### 2024/3/7

1.  实验了利用lstm进行number_recall任务相较于STM的优缺点。STM相较于lstm收敛速度更快。`lstm_number_recall.py`文件中的代码是相关实现。在该参数下，lstm在50 epoch下完成收敛，而STM能在30 epoch下完成收敛。可能是任务较为简单，无法体现两者之间的差距。在实验过程中，可以观察到lstm在前25 epoch中loss基本无改变，准确率也是低于0.2，在25 epoch后loss开始迅速减小，并在接下来25 epoch中迅速收敛至0。除此之外，还尝试在输入最后一组数据并求得输出之前，拼接一个提示向量，用于表示接下来需要输出一个预测值，但在实际过程中，添加了这个向量反而使得模型收敛速度变慢，并无预料中的加速收敛和增大准确率的作用（这里任务过于简单，都能做到100%正确，因此可能无法体现这个提示向量的作用）。

总结：STM并未表现出明显优于lstm的地方，建议之后加大任务难度重新测试。rnn的训练需要更多的epoch，前期loss下降不明显是正常现象。

补充：后续添加了重复部分和提示向量的消融实验。添加随机重复部分后依旧能在原模型基础上在5 epoch内收敛。消融实验显示提示向量没有作用，直接删除也不会对原模型表现有所影响。

2.  猜想部分：关于HumanAI，目前认为Memory的整理与优化是关键，不过目前还没有找到合适的方法。任务选择上也没有合适的任务。简单情况下，HumanAI的任务应该在处理data并存入Memory后能重复提取Memory内容直到输出可以结束，这里其实可以选择是重复处理Memory内容还是重复读取data来实现，重复读取Memory对隐空间的要求更高，重复读取data则对训练方式要求更高。当前重点是Memory的内容与优化方式。